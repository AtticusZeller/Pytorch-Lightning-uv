model:
  activation: relu
  dropout: null
  n_layer_1: 128
  n_layer_2: 256
  name: MLP
